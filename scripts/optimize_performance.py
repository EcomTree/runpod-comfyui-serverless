#!/usr/bin/env python3
"""
ComfyUI Performance Optimization Script
Applies PyTorch 2.0+ optimizations for 20-30% speed boost
"""

import os
import sys
import torch
import argparse
from pathlib import Path


def check_cuda_available():
    """Check if CUDA is available"""
    if not torch.cuda.is_available():
        print("‚ö†Ô∏è CUDA not available. Performance optimizations require GPU.")
        return False
    
    cuda_version = torch.version.cuda
    device_name = torch.cuda.get_device_name(0)
    device_count = torch.cuda.device_count()
    
    print(f"‚úÖ CUDA {cuda_version} available")
    print(f"üéÆ GPU: {device_name}")
    print(f"üìä Devices: {device_count}")
    
    return True


def check_torch_compile_support():
    """Check if torch.compile is available (PyTorch 2.0+)"""
    try:
        torch_version = torch.__version__
        major, minor = map(int, torch_version.split('.')[:2])
        
        if major >= 2:
            print(f"‚úÖ PyTorch {torch_version} supports torch.compile")
            return True
        else:
            print(f"‚ö†Ô∏è PyTorch {torch_version} does not support torch.compile (requires 2.0+)")
            return False
    except Exception as e:
        print(f"‚ùå Error checking PyTorch version: {e}")
        return False


def apply_cuda_optimizations():
    """Apply CUDA-specific optimizations"""
    print("\nüîß Applying CUDA optimizations...")
    
    optimizations = []
    
    # Enable TF32 for Ampere+ GPUs (20% speedup)
    if hasattr(torch.backends.cuda, 'matmul'):
        torch.backends.cuda.matmul.allow_tf32 = True
        optimizations.append("TF32 matmul enabled")
    
    if hasattr(torch.backends.cudnn, 'allow_tf32'):
        torch.backends.cudnn.allow_tf32 = True
        optimizations.append("TF32 cuDNN enabled")
    
    # Enable cuDNN benchmarking for consistent input sizes
    torch.backends.cudnn.benchmark = True
    optimizations.append("cuDNN benchmark enabled")
    
    # Set cuDNN deterministic to False for better performance
    torch.backends.cudnn.deterministic = False
    optimizations.append("cuDNN non-deterministic mode")
    
    for opt in optimizations:
        print(f"  ‚úì {opt}")
    
    return optimizations


def apply_memory_optimizations():
    """Apply memory management optimizations"""
    print("\nüíæ Applying memory optimizations...")
    
    optimizations = []
    
    # Set CUDA memory allocator configuration
    if torch.cuda.is_available():
        # Expandable segments for better memory utilization
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'
        optimizations.append("Expandable segments enabled")
        
        # Enable memory efficient attention if available
        if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
            optimizations.append("Scaled dot product attention available")
    
    for opt in optimizations:
        print(f"  ‚úì {opt}")
    
    return optimizations


def get_compile_backend():
    """Determine the best torch.compile backend"""
    # Default backends by priority
    backends = ['inductor', 'aot_autograd', 'cudagraphs']
    
    # Try to determine the best backend
    if torch.cuda.is_available():
        compute_capability = torch.cuda.get_device_capability(0)
        major, minor = compute_capability
        
        print(f"üîç GPU Compute Capability: {major}.{minor}")
        
        # Ampere and newer (SM 8.0+) work best with inductor
        if major >= 8:
            print("‚úÖ Recommended backend: inductor (Ampere+ GPU)")
            return 'inductor'
        # Volta and Turing (SM 7.x) work well with aot_autograd
        elif major >= 7:
            print("‚úÖ Recommended backend: aot_autograd (Volta/Turing GPU)")
            return 'aot_autograd'
    
    return 'inductor'


def generate_startup_flags(enable_compile=True, backend='inductor'):
    """Generate ComfyUI startup flags for optimizations"""
    flags = []
    
    print("\nüöÄ Recommended ComfyUI startup flags:")
    
    # Basic optimization flags
    flags.extend([
        '--preview-method', 'auto',
        '--cache-lru', '3',
        '--normalvram'  # Good balance for most use cases
    ])
    
    # Add compile flag if supported
    if enable_compile:
        flags.append('--enable-compile')
        print(f"  ‚úì torch.compile enabled with backend: {backend}")
    
    # Environment variables
    env_vars = {
        'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512,expandable_segments:True',
        'TORCH_ALLOW_TF32_CUBLAS_OVERRIDE': '1',
        'PYTORCH_JIT': '1'
    }
    
    print("\nüìã Recommended environment variables:")
    for key, value in env_vars.items():
        print(f"  export {key}={value}")
    
    print("\nüìù Full startup command:")
    print(f"  python main.py {' '.join(flags)}")
    
    return flags, env_vars


def run_benchmark(model_path=None):
    """Run a simple benchmark to test optimizations"""
    print("\n‚è±Ô∏è Running benchmark...")
    
    try:
        import time
        
        # Simple matrix multiplication benchmark
        size = 4096
        iterations = 100
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Warmup
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        for _ in range(10):
            _ = torch.matmul(a, b)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        # Benchmark
        start = time.time()
        for _ in range(iterations):
            c = torch.matmul(a, b)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        elapsed = time.time() - start
        operations = size * size * size * 2 * iterations  # FLOPS
        gflops = (operations / elapsed) / 1e9
        
        print(f"‚úÖ Benchmark completed:")
        print(f"  Matrix size: {size}x{size}")
        print(f"  Iterations: {iterations}")
        print(f"  Time: {elapsed:.2f}s")
        print(f"  Performance: {gflops:.2f} GFLOPS")
        
        return True
    
    except Exception as e:
        print(f"‚ùå Benchmark failed: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description='ComfyUI Performance Optimization')
    parser.add_argument('--check', action='store_true', help='Check system capabilities')
    parser.add_argument('--apply', action='store_true', help='Apply optimizations')
    parser.add_argument('--benchmark', action='store_true', help='Run performance benchmark')
    parser.add_argument('--generate-flags', action='store_true', help='Generate startup flags')
    parser.add_argument('--backend', type=str, default='auto', 
                       choices=['auto', 'inductor', 'aot_autograd', 'cudagraphs'],
                       help='torch.compile backend')
    parser.add_argument('--no-compile', action='store_true', help='Disable torch.compile')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ComfyUI Performance Optimization")
    print("=" * 60)
    
    # Check system
    if args.check or not any([args.apply, args.benchmark, args.generate_flags]):
        print("\nüìä System Check:")
        cuda_available = check_cuda_available()
        compile_supported = check_torch_compile_support()
        
        if cuda_available:
            print("\nüîç CUDA Device Info:")
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"  Device {i}: {props.name}")
                print(f"    Compute Capability: {props.major}.{props.minor}")
                print(f"    Total Memory: {props.total_memory / 1024**3:.2f} GB")
                print(f"    Multi Processors: {props.multi_processor_count}")
    
    # Apply optimizations
    if args.apply:
        if check_cuda_available():
            apply_cuda_optimizations()
            apply_memory_optimizations()
            print("\n‚úÖ Optimizations applied successfully!")
        else:
            print("\n‚ö†Ô∏è Cannot apply optimizations without CUDA")
            sys.exit(1)
    
    # Run benchmark
    if args.benchmark:
        run_benchmark()
    
    # Generate flags
    if args.generate_flags:
        backend = args.backend if args.backend != 'auto' else get_compile_backend()
        enable_compile = not args.no_compile and check_torch_compile_support()
        generate_startup_flags(enable_compile, backend)
    
    print("\n" + "=" * 60)
    print("üí° Tips:")
    print("  - Use --enable-compile for 20-30% faster inference")
    print("  - TF32 provides ~20% speedup on Ampere+ GPUs")
    print("  - Use --normalvram for optimal memory usage")
    print("  - Consider --cache-lru 3 for faster repeated operations")
    print("=" * 60)


if __name__ == "__main__":
    main()
