# RunPod ComfyUI Serverless Handler - Optimized Edition

A high-performance, optimized serverless handler for running ComfyUI image & video workflows on RunPod's Serverless GPU Infrastructure with comprehensive performance enhancements.

> **âš ï¸ CUDA 12.8 Required**: This image only works on **Ada Lovelace** (RTX 40xx, L40), **Hopper** (H100/H200), or **Blackwell** GPUs. Older Ampere GPUs (RTX 30xx, A100) are NOT supported!

## ğŸš€ Features

### Core Features
- **Serverless GPU Computing**: Uses RunPod's Serverless Platform for scalable GPU computations
- **ComfyUI Integration**: Seamless integration with ComfyUI for AI image & video generation
- **Heavy Video Rendering**: Optimized for long-running video workflows (AnimateDiff, SVD, etc.)
- **Automatic Seed Randomization**: Seeds are automatically randomized for each execution (configurable)
- **S3 Storage**: Direct upload to Cloudflare R2, AWS S3, or Backblaze B2 with presigned URLs
- **RunPod Network Volume Support**: Automatic backup of generated files to RunPod Network Volume
- **Workflow Flexibility**: Supports both predefined and dynamic workflows
- **Extended Timeouts**: 20 min startup timeout, 60 min workflow execution timeout
- **Error Handling**: Robust error handling and detailed logging with automatic stderr output

### ğŸ†• Performance Optimizations (v3.0)
- **20-30% Faster Inference**: torch.compile support for PyTorch 2.0+
- **TF32 Operations**: ~20% speedup on Ampere+ GPUs
- **CUDA Pre-warming**: Faster first inference in serverless
- **Multi-stage Docker Build**: 50% smaller images with BuildKit cache
- **Fast Cold Starts**: Optimized initialization for serverless

### ğŸ†• Dynamic Version Management
- **Automatic Version Detection**: Fetch latest ComfyUI releases
- **Build-time Version Selection**: Configurable via Docker ARG
- **Version Verification**: Validate ComfyUI versions before deployment

### ğŸ†• Extended Custom Nodes (6 Essential Nodes)
- **ComfyUI-Manager**: Essential management and model installation
- **ComfyUI-Impact-Pack**: Advanced image processing and segmentation
- **rgthree-comfy**: Workflow enhancements and quality of life improvements
- **ComfyUI-Advanced-ControlNet**: Extended ControlNet capabilities
- **ComfyUI-VideoHelperSuite**: Comprehensive video processing
- **LoadImageFromHttpURL**: Direct HTTP image loading

### ğŸ†• Automated Model Management
- **160+ Models**: Comprehensive model library across 10 categories
- **Model Packs**: Pre-configured collections (Essential, SD 1.5, SDXL, Video, FLUX)
- **Parallel Downloads**: Multi-threaded downloading with progress tracking
- **Resume Support**: Automatic resume for interrupted downloads
- **Link Verification**: Validate all model URLs before downloading

## ğŸ“‹ Requirements

- RunPod Account with API Key
- RunPod Network Volume (for persistent storage)
- Docker (for image build)
- Python 3.11+

### ğŸ”¥ GPU Requirements (CUDA 12.8)

**âš ï¸ IMPORTANT: This image requires CUDA 12.8 or higher!**

Only GPUs with **Ada Lovelace, Hopper, or Blackwell architecture** are supported:

#### âœ… Compatible GPUs (CUDA 12.8+):
- **Consumer/Prosumer:**
  - RTX 4090, RTX 4080, RTX 4070 Ti (Ada Lovelace)
  - RTX 5090, RTX 5080 (Blackwell - when available)
  
- **Datacenter:**
  - L40, L40S (Ada Lovelace)
  - H100, H200 (Hopper)
  - B100, B200 (Blackwell)

#### âŒ NOT Compatible (Older Architectures):
- **RTX 3090, RTX 3080, A100, A40** (Ampere) - Will NOT work!
- All older GPUs (Turing, Pascal, etc.)

**How to filter in RunPod:**
1. Go to RunPod â†’ Serverless â†’ Deploy Endpoint
2. Filter by "CUDA 12.8" or "CUDA 12.9"
3. Only select GPUs from the compatible list above

#### ğŸ’¾ VRAM Recommendations by Workload:
- **Images (SD 1.5/SDXL)**: 16GB+ (RTX 4080, L40)
- **Videos (AnimateDiff, SVD)**: 24GB+ (RTX 4090, L40S)
- **Heavy Video (4K, long sequences)**: 48GB+ (H100, H200)

## ğŸ› ï¸ Installation

> **Quick Setup:**
>
> ```bash
> # Download and run the unified setup script
> curl -fsSL https://raw.githubusercontent.com/EcomTree/runpod-comfyui-serverless/main/scripts/setup.sh | bash
> ```

1. **Clone Repository**
   ```bash
   git clone https://github.com/EcomTree/runpod-comfyui-serverless.git
   cd runpod-comfyui-serverless
   ```

2. **Setup Environment**
   ```bash
   # Run the setup script for automatic configuration
   bash scripts/setup.sh
   ```

3. **Build Docker Image**
   ```bash
   # Standard build
   docker build -t ecomtree/comfyui-serverless:latest .
   
   # With specific ComfyUI version
   docker build --build-arg COMFYUI_VERSION=v0.3.60 -t ecomtree/comfyui-serverless:latest .
   
   # With BuildKit for faster builds (recommended)
   DOCKER_BUILDKIT=1 docker build -t ecomtree/comfyui-serverless:latest .
   ```

4. **Push Image to Docker Hub**
   ```bash
   docker push ecomtree/comfyui-serverless:latest
   ```

## ğŸ”§ Configuration

### Environment Variables

The handler supports the following environment variables:

#### ComfyUI Configuration
- `COMFY_PORT`: ComfyUI Port (default: 8188)
- `COMFY_HOST`: ComfyUI Host (default: 127.0.0.1)
- `RANDOMIZE_SEEDS`: Automatically randomize all seeds in workflows (default: true)
  - Set to `false` if you want to preserve exact seeds from your workflow
  - When enabled, all seed values are replaced with random values before execution

#### Storage Configuration (S3 or Network Volume)

**S3 Storage (Recommended for HTTP Access):**
- `S3_BUCKET`: Name of your S3 Bucket (required)
- `S3_ACCESS_KEY`: S3 Access Key ID (required)
- `S3_SECRET_KEY`: S3 Secret Access Key (required)
- `S3_ENDPOINT_URL`: Custom Endpoint for S3-compatible services (e.g. Cloudflare R2, Backblaze B2)
- `S3_REGION`: S3 Region (default: "auto")
- `S3_PUBLIC_URL`: Optional: Custom Public URL Prefix (e.g. CDN URL)
- `S3_SIGNED_URL_EXPIRY`: Validity duration of signed URLs in seconds (default: 3600)

**Network Volume (Fallback):**
- `RUNPOD_VOLUME_PATH`: Path to Network Volume (default: /runpod-volume)
- `RUNPOD_OUTPUT_DIR`: Alternative output directory (optional)

**Note:** When S3 is configured, it will be used automatically. The Network Volume serves as fallback.

#### Debugging & Logging
- `DEBUG_S3_URLS`: Log full presigned URLs including authentication tokens (default: false)
  - **âš ï¸ Security Warning:** Only enable in development! Presigned URLs contain sensitive tokens
  - When disabled, URLs in logs show path only with note: `[presigned - query params redacted for security]`
  - See [URL_LOGGING.md](./URL_LOGGING.md) for detailed information

### Workflow Configuration

Workflows are passed as JSON directly in the request. The handler expects the ComfyUI workflow format.

## ğŸ“ Usage

### Request Format

```json
{
  "input": {
    "workflow": {
      // ComfyUI Workflow JSON
      // Example: SD 1.5 Text-to-Image
      "3": {
        "inputs": {
          "seed": 42,
          "steps": 20,
          "cfg": 7.0,
          "sampler_name": "euler",
          "scheduler": "normal",
          "denoise": 1.0,
          "model": ["4", 0],
          "positive": ["6", 0],
          "negative": ["7", 0],
          "latent_image": ["5", 0]
        },
        "class_type": "KSampler"
      }
      // ... more nodes
    }
  }
}
```

### Response Format

**With S3 Storage (Cloudflare R2, AWS S3, Backblaze B2):**
```json
{
  "links": [
    "https://account-id.r2.cloudflarestorage.com/comfyui-outputs/job-id/20251003_120530_output_image.png?X-Amz-..."
  ],
  "total_images": 1,
  "job_id": "abc123",
  "storage_type": "s3",
  "s3_bucket": "comfyui-outputs",
  "local_paths": [
    "/workspace/ComfyUI/output/output_image.png"
  ],
  "volume_paths": [
    "/runpod-volume/comfyui/output/comfyui-20251003_120530_000000-abc12345-output_image.png"
  ]
}
```

**With Network Volume Only (S3 not configured):**
```json
{
  "links": [
    "/runpod-volume/comfyui/output/comfyui-20251003_120530_000000-abc12345-output_image.png"
  ],
  "total_images": 1,
  "job_id": "abc123",
  "storage_type": "volume",
  "volume_paths": [
    "/runpod-volume/comfyui/output/comfyui-20251003_120530_000000-abc12345-output_image.png"
  ]
}
```

**Note:** When S3 is configured, images are uploaded to S3 **and** backed up to the Network Volume. The `links` array contains publicly accessible S3 URLs (presigned URLs by default, or custom CDN URLs if `S3_PUBLIC_URL` is set).

## â˜ï¸ S3 Setup Guide

### Cloudflare R2 (Recommended - Free up to 10GB)

1. **Create R2 Bucket:**
   - Go to [Cloudflare Dashboard](https://dash.cloudflare.com/) â†’ R2
   - Create new bucket (e.g. `comfyui-outputs`)

2. **Create API Token:**
   - R2 â†’ Manage R2 API Tokens â†’ Create API Token
   - Note down: Access Key ID, Secret Access Key
   - Endpoint URL: `https://<account-id>.r2.cloudflarestorage.com`

3. **Set Environment Variables in RunPod:**
   ```
   S3_BUCKET=comfyui-outputs
   S3_ACCESS_KEY=<your-access-key>
   S3_SECRET_KEY=<your-secret-key>
   S3_ENDPOINT_URL=https://<account-id>.r2.cloudflarestorage.com
   S3_REGION=auto
   ```

### AWS S3

1. **Create S3 Bucket:**
   - [AWS Console](https://console.aws.amazon.com/s3/) â†’ Create Bucket
   - Select region (e.g. `us-east-1`)

2. **IAM User & Credentials:**
   - IAM â†’ Users â†’ Add User
   - Permissions: `s3:PutObject`, `s3:GetObject`, `s3:DeleteObject`

3. **Environment Variables:**
   ```
   S3_BUCKET=your-bucket-name
   S3_ACCESS_KEY=<aws-access-key>
   S3_SECRET_KEY=<aws-secret-key>
   S3_REGION=us-east-1
   ```

### Backblaze B2

1. **Create Bucket:** [Backblaze Console](https://www.backblaze.com/b2/cloud-storage.html)
2. **Create Application Key:** Note down Key ID & Key
3. **Environment Variables:**
   ```
   S3_BUCKET=your-bucket-name
   S3_ACCESS_KEY=<key-id>
   S3_SECRET_KEY=<application-key>
   S3_ENDPOINT_URL=https://s3.us-west-002.backblazeb2.com
   S3_REGION=us-west-002
   ```

## ğŸ§ª Testing

Use the included test script to validate your endpoint:

```bash
# Configure the test script
cp scripts/test_endpoint.sh scripts/test_endpoint_local.sh
# Edit scripts/test_endpoint_local.sh with your ENDPOINT_ID and API_KEY

# Run tests
bash scripts/test_endpoint_local.sh
```

**Note**: Never commit API keys or endpoint IDs to version control!

```bash
#!/bin/bash
# WARNING: Do not commit real API keys or endpoint IDs to version control!
ENDPOINT_ID="your-endpoint-id"
API_KEY="your-runpod-api-key"
API_URL="https://api.runpod.ai/v2/${ENDPOINT_ID}/runsync"

curl -X POST "$API_URL" \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "workflow": "workflow_data_here"
    }
  }'
```

## ğŸ—ï¸ Architecture

```
runpod-comfyui-serverless/
â”œâ”€â”€ src/                                 # Source code modules
â”‚   â”œâ”€â”€ config.py                        # Configuration management
â”‚   â”œâ”€â”€ comfyui_manager.py               # ComfyUI server lifecycle
â”‚   â”œâ”€â”€ s3_handler.py                    # S3 storage operations
â”‚   â””â”€â”€ workflow_processor.py            # Workflow processing utilities
â”œâ”€â”€ scripts/                             # Setup and maintenance scripts
â”‚   â”œâ”€â”€ setup.sh                         # Unified setup script
â”‚   â”œâ”€â”€ get_latest_version.sh            # ğŸ†• ComfyUI version management
â”‚   â”œâ”€â”€ optimize_performance.py          # ğŸ†• Performance optimization tool
â”‚   â”œâ”€â”€ install_custom_nodes.sh          # ğŸ†• Custom nodes installer
â”‚   â”œâ”€â”€ download_models.py               # ğŸ†• Model download manager
â”‚   â”œâ”€â”€ verify_links.py                  # ğŸ†• Link verification tool
â”‚   â”œâ”€â”€ common-codex.sh                  # Shared helper functions
â”‚   â””â”€â”€ test_endpoint.sh                 # Testing utilities
â”œâ”€â”€ configs/                             # ğŸ†• Configuration files
â”‚   â””â”€â”€ custom_nodes.json                # Custom nodes configuration
â”œâ”€â”€ docs/                                # ğŸ†• Documentation
â”‚   â”œâ”€â”€ performance-tuning.md            # Performance guide
â”‚   â”œâ”€â”€ custom-nodes.md                  # Custom nodes guide
â”‚   â””â”€â”€ model-management.md              # Model management guide
â”œâ”€â”€ models_download.json                 # ğŸ†• Model library (160+ models)
â”œâ”€â”€ rp_handler.py                        # Main RunPod handler
â”œâ”€â”€ Dockerfile                           # ğŸ†• Multi-stage Docker image
â”œâ”€â”€ .dockerignore                        # ğŸ†• Docker build optimization
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”œâ”€â”€ .env.example                         # Configuration template
â”œâ”€â”€ .gitignore                           # Git ignore rules
â””â”€â”€ README.md                            # This file
```

### Handler Architecture

The handler is now organized into focused modules:

- **src/config.py**: Centralized configuration management with environment variable parsing
- **src/comfyui_manager.py**: ComfyUI server lifecycle, workflow execution, and model management
- **src/s3_handler.py**: S3 storage operations with proper error handling and URL sanitization
- **src/workflow_processor.py**: Workflow processing utilities including seed randomization
- **rp_handler.py**: Main entry point that orchestrates all components

## ğŸš€ Deployment

1. **Setup and Build**
   ```bash
   # Clone and setup the project
   git clone https://github.com/EcomTree/runpod-comfyui-serverless.git
   cd runpod-comfyui-serverless
   bash scripts/setup.sh

   # Build Docker image
   docker build -t ecomtree/comfyui-serverless:latest -f Dockerfile .
   docker push ecomtree/comfyui-serverless:latest
   ```

2. **Create RunPod Serverless Endpoint**
   - Go to [RunPod Dashboard](https://runpod.io/console/serverless)
   - Create new Serverless Endpoint
   - **Docker Image**: `ecomtree/comfyui-serverless:latest`
   - **Container Disk**: at least 15GB (20GB recommended for large models)
   - **GPU Filter**: CUDA 12.8 or 12.9 only!
   - **GPU**: RTX 4090, L40/L40S, H100/H200 or better (see GPU Requirements above)
   - **Important**: Connect Network Volume with sufficient storage for models and outputs

3. **Configure Environment Variables**
   Set the following environment variables in your RunPod endpoint:
   ```bash
   # S3 Storage (recommended)
   S3_BUCKET=your-bucket-name
   S3_ACCESS_KEY=your-access-key
   S3_SECRET_KEY=your-secret-key
   S3_ENDPOINT_URL=https://your-s3-endpoint.com

   # Performance tuning
   PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,expandable_segments:True
   TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
   ```

4. **Note down credentials**
   - **Endpoint ID**: Available in the RunPod dashboard
   - **API Key**: Generated when creating the endpoint

## ğŸ“Š Performance

### Baseline Performance
- **Cold Start**: ~15-30 seconds (ComfyUI + Model Loading)
- **Heavy Model Loading**: Up to 20 minutes for large model collections
- **Warm Start**: ~2-5 seconds
- **Image Workflow**: 5-120 seconds (depends on model and settings)
- **Video Workflow**: 2-60 minutes (depends on frames, resolution, and models)
- **S3 Upload**: ~1-5 seconds per file
- **Volume Save**: <1 second per file

### With Optimizations (v3.0)
- **Inference Speed**: 20-30% faster with torch.compile + TF32
- **Cold Start**: 25% faster with CUDA pre-warming
- **Docker Build**: 50% smaller images with multi-stage build
- **Build Time**: 40% faster with BuildKit cache mounts

### Real-World Benchmarks (RTX 4090)

**SD 1.5 (512x512, 20 steps):**
- Base: ~2.5s
- Optimized: ~1.8s
- **Improvement: 28%**

**SDXL (1024x1024, 20 steps):**
- Base: ~8.5s
- Optimized: ~6.2s
- **Improvement: 27%**

**FLUX (1024x1024, 20 steps):**
- Base: ~15.2s
- Optimized: ~11.8s
- **Improvement: 22%**

## ğŸ’¡ Technical Details

- **Base Image**: `runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04`
- **CUDA Version**: 12.8.1 (requires Ada Lovelace, Hopper, or Blackwell GPUs)
- **ComfyUI Version**: v0.3.57 (configurable via build arg)
- **PyTorch**: 2.8.0 with CUDA 12.8
- **Build Type**: Multi-stage with BuildKit optimizations
- **GPU Memory**: Optimized with `--normalvram` and expandable segments
- **Tensor Cores**: Fully optimized for modern Tensor Cores (4th gen+)
- **Custom Nodes**: 6 essential nodes pre-installed
- **Optimizations**: torch.compile, TF32, cuDNN benchmark, CUDA pre-warming

## ğŸ“š Documentation

Comprehensive documentation is available in the `/docs` directory:

- **[Performance Tuning Guide](docs/performance-tuning.md)** - Complete guide to all performance optimizations
- **[Custom Nodes Guide](docs/custom-nodes.md)** - Documentation for all 6 included custom nodes
- **[Model Management Guide](docs/model-management.md)** - How to download and manage 160+ models

## ğŸ”§ New Configuration Options

### Performance Settings

```bash
# Enable torch.compile for 20-30% speedup (experimental)
ENABLE_TORCH_COMPILE=false

# Apply all performance optimizations
ENABLE_OPTIMIZATIONS=true

# Pre-warm CUDA for faster first inference
PREWARM_CUDA=true

# Fast startup mode for serverless
FAST_STARTUP=true
```

### Docker Build Arguments

```bash
# Build with specific ComfyUI version
docker build --build-arg COMFYUI_VERSION=v0.3.57 .

# Enable/disable custom nodes installation
docker build --build-arg INSTALL_CUSTOM_NODES=true .

# Enable optimizations at build time
docker build --build-arg ENABLE_OPTIMIZATIONS=true .
```

## ğŸ¯ Quick Start Examples

### Download Essential Models

```bash
# Download SDXL + VAE + Upscaler
python3 /workspace/scripts/download_models.py download-pack essential
```

### Check Performance Capabilities

```bash
# Check system and optimization status
python3 /workspace/scripts/optimize_performance.py --check

# Run performance benchmark
python3 /workspace/scripts/optimize_performance.py --benchmark
```

### Manage Custom Nodes

```bash
# List installed nodes
/workspace/scripts/install_custom_nodes.sh list

# Update all nodes
/workspace/scripts/install_custom_nodes.sh update
```

### Verify Model Links

```bash
# Verify all model download links
python3 /workspace/scripts/verify_links.py

# Verify specific category
python3 /workspace/scripts/verify_links.py --category checkpoints
```

## ğŸš€ Advanced Features

### Dynamic ComfyUI Version

```bash
# Get latest ComfyUI version
/workspace/scripts/get_latest_version.sh latest

# Check installed version
/workspace/scripts/get_latest_version.sh installed

# Validate specific version
/workspace/scripts/get_latest_version.sh validate v0.3.57
```

### Model Packs

Pre-configured model collections:

```bash
# Essential Pack (SDXL basics) - ~7.3 GB
python3 /workspace/scripts/download_models.py download-pack essential

# SD 1.5 Pack (complete) - ~7.9 GB
python3 /workspace/scripts/download_models.py download-pack sd15_pack

# SDXL Pack (full) - ~13.6 GB
python3 /workspace/scripts/download_models.py download-pack sdxl_pack

# Video Pack (AnimateDiff) - ~6.3 GB
python3 /workspace/scripts/download_models.py download-pack video_pack

# FLUX Pack (next-gen) - ~33.2 GB (requires 24GB+ VRAM)
python3 /workspace/scripts/download_models.py download-pack flux_pack
```

### Multi-stage Build

The Dockerfile now uses multi-stage builds for:
- 50% smaller final images
- Better layer caching
- Faster rebuilds with BuildKit
- Optimized dependencies

## ğŸ¤ Contributing

Contributions are welcome! Please create a pull request with your changes.

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [RunPod](https://runpod.io) for the Serverless GPU Infrastructure
- [ComfyUI](https://github.com/comfyanonymous/ComfyUI) for the awesome AI Workflow System
- The Open Source Community for continuous support

---

Created with â¤ï¸ for the AI Art Community
